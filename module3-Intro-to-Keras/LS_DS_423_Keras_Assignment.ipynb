{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.keras.datasets.boston_housing' from '/Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages/tensorflow/keras/datasets/boston_housing/__init__.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.2 42.3 50.  21.1 17.7]\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape,X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs normalizing\n",
    "from tensorflow.keras.utils import normalize\n",
    "X_train = normalize(X_train)\n",
    "y_train = normalize(y_train)[0]\n",
    "X_test = normalize(X_test)\n",
    "y_test = normalize(y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.41189924e-03 0.00000000e+00 1.59296858e-02 0.00000000e+00\n",
      "  1.05284655e-03 1.20196720e-02 1.79453585e-01 7.78264954e-03\n",
      "  7.82785541e-03 6.00787902e-01 4.10962409e-02 7.76718953e-01\n",
      "  3.66343633e-02]\n",
      " [4.07923050e-05 1.54587284e-01 3.80378407e-03 0.00000000e+00\n",
      "  7.77620881e-04 1.42595058e-02 2.94184285e-02 1.17486336e-02\n",
      "  3.74757051e-03 6.52077269e-01 2.75446433e-02 7.40857215e-01\n",
      "  5.82747215e-03]\n",
      " [6.34505528e-03 0.00000000e+00 2.34463745e-02 0.00000000e+00\n",
      "  8.17384658e-04 6.43803764e-03 1.29537981e-01 1.72609359e-03\n",
      "  3.10891154e-02 8.62722952e-01 2.61666721e-02 4.86441025e-01\n",
      "  4.22293817e-03]\n",
      " [8.65407330e-05 0.00000000e+00 1.13392175e-02 0.00000000e+00\n",
      "  1.12518247e-03 1.31897603e-02 7.53763011e-02 1.30768051e-02\n",
      "  1.09241016e-02 4.89399752e-01 4.41333705e-02 8.67155186e-01\n",
      "  1.75004108e-02]\n",
      " [4.74343543e-03 0.00000000e+00 2.32476643e-02 0.00000000e+00\n",
      "  9.15778156e-04 8.18934295e-03 1.13541078e-01 3.29718668e-03\n",
      "  3.08256322e-02 8.55411293e-01 2.59449071e-02 5.02753217e-01\n",
      "  1.88164796e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])\n",
    "# looks good, now it can be modelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, AlphaDropout\n",
    "from tensorflow.keras import regularizers\n",
    "model = Sequential()\n",
    "model.add(Dense(128,activation='relu',input_dim=13))\n",
    "model.add(AlphaDropout(rate=1))\n",
    "model.add(Dense(128,activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(AlphaDropout(rate=1))\n",
    "model.add(Dense(1,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "stop = EarlyStopping(monitor='val_mae', min_delta=0.0001, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 1.1511 - mse: 4.8857e-04 - val_loss: 0.9950 - val_mse: 0.0038\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.8789 - mse: 2.9318e-04 - val_loss: 0.7528 - val_mse: 0.0026\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6617 - mse: 2.7668e-04 - val_loss: 0.5637 - val_mse: 0.0027\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4928 - mse: 2.5798e-04 - val_loss: 0.4185 - val_mse: 0.0031\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3635 - mse: 2.5457e-04 - val_loss: 0.3081 - val_mse: 0.0033\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.2657 - mse: 2.5961e-04 - val_loss: 0.2245 - val_mse: 0.0030\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1924 - mse: 2.4333e-04 - val_loss: 0.1627 - val_mse: 0.0033\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1379 - mse: 2.4758e-04 - val_loss: 0.1163 - val_mse: 0.0028\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0978 - mse: 2.4292e-04 - val_loss: 0.0828 - val_mse: 0.0028\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0686 - mse: 2.6237e-04 - val_loss: 0.0590 - val_mse: 0.0033\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0476 - mse: 2.5829e-04 - val_loss: 0.0418 - val_mse: 0.0035\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0327 - mse: 2.5303e-04 - val_loss: 0.0291 - val_mse: 0.0030\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0222 - mse: 2.5095e-04 - val_loss: 0.0209 - val_mse: 0.0034\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0149 - mse: 2.4783e-04 - val_loss: 0.0147 - val_mse: 0.0031\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0099 - mse: 2.6577e-04 - val_loss: 0.0104 - val_mse: 0.0028\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0066 - mse: 2.7652e-04 - val_loss: 0.0082 - val_mse: 0.0032\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 2.9486e-04 - val_loss: 0.0073 - val_mse: 0.0041\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0029 - mse: 2.7553e-04 - val_loss: 0.0053 - val_mse: 0.0033\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0019 - mse: 2.5899e-04 - val_loss: 0.0040 - val_mse: 0.0028\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 2.8726e-04 - val_loss: 0.0041 - val_mse: 0.0034\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 8.8968e-04 - mse: 2.6462e-04 - val_loss: 0.0036 - val_mse: 0.0031\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 6.5315e-04 - mse: 2.6902e-04 - val_loss: 0.0039 - val_mse: 0.0036\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 5.1095e-04 - mse: 2.7795e-04 - val_loss: 0.0040 - val_mse: 0.0038\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4.4476e-04 - mse: 2.9735e-04 - val_loss: 0.0030 - val_mse: 0.0029\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3.6362e-04 - mse: 2.7165e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 3.2401e-04 - mse: 2.6481e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3.1943e-04 - mse: 2.7499e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 3.0549e-04 - mse: 2.7487e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.8779e-04 - mse: 2.6178e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 3.3405e-04 - mse: 3.0876e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.2550e-04 - mse: 3.0175e-04 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.0349e-04 - mse: 2.8398e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1098e-04 - mse: 2.9325e-04 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1397e-04 - mse: 2.9292e-04 - val_loss: 0.0031 - val_mse: 0.0030\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1115e-04 - mse: 2.9223e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8123e-04 - mse: 2.6163e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8521e-04 - mse: 2.6776e-04 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8706e-04 - mse: 2.6579e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.8468e-04 - mse: 2.6459e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 2.8870e-04 - mse: 2.6999e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2.8901e-04 - mse: 2.7370e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2.9793e-04 - mse: 2.7714e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.8065e-04 - mse: 2.6285e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.7782e-04 - mse: 2.5940e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.7818e-04 - mse: 2.6099e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2.9114e-04 - mse: 2.7285e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.7855e-04 - mse: 2.5971e-04 - val_loss: 0.0038 - val_mse: 0.0037\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2.8045e-04 - mse: 2.6263e-04 - val_loss: 0.0031 - val_mse: 0.0030\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.7238e-04 - mse: 2.5540e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.2214e-04 - mse: 2.9997e-04 - val_loss: 0.0024 - val_mse: 0.0024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13c774ee0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "model.fit(X_train, y_train, epochs=50, \n",
    "          validation_data=(X_test,y_test),\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9280667553297117"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare it to a linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS:\n",
    "# the neural network is very good.\n",
    "# the linear regression is not very good.\n",
    "# the neural network does better than the linear regression.\n",
    "# i think.\n",
    "# I mean technically the NN is using MSE and the linreg is using R^2.\n",
    "# but the NN is hitting an MSE of 2.5511e-04,\n",
    "# whereas the linear regression has an R^2 of almost 1.\n",
    "# that's not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- The number of nodes in your output layer should equal the number of classes you want to predict for Fashion-MNIST.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "logdir = os.path.join(\"logs2\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab mnist data\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "stop = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages (from keras) (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages (from keras) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages (from keras) (1.18.4)\n",
      "Requirement already satisfied: h5py in /Users/anitasharma/opt/anaconda3/envs/U4-S2-NNF/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# set up model\n",
    "model3 = Sequential([\n",
    "    Flatten(input_shape=(28,28)), # flatten the data coming in\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 3.6789 - accuracy: 0.8805 - val_loss: 1.8827 - val_accuracy: 0.9264\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 1.1711 - accuracy: 0.9466 - val_loss: 0.6961 - val_accuracy: 0.9502\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4917 - accuracy: 0.9524 - val_loss: 0.3569 - val_accuracy: 0.9571\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3411 - accuracy: 0.9505 - val_loss: 0.3333 - val_accuracy: 0.9476\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3066 - accuracy: 0.9524 - val_loss: 0.2993 - val_accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x105c04130>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "model3.fit(X_train, y_train, epochs=50, \n",
    "          validation_data=(X_test,y_test),\n",
    "          callbacks=[tensorboard_callback, stop]) # early stop callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NNF (Python3)",
   "language": "python",
   "name": "u4-s2-nnf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
